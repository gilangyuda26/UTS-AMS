{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1rAsSg92fVlErxhQfQWTR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gilangyuda26/UTS-AMS/blob/main/AMS_A11_2019_11966.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zsh6mG8BfDBg"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Sentimen'], axis=1)"
      ],
      "metadata": {
        "id": "xcNhAC7SfTZk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "HcmIGKJaf_hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "odWI4PWMgMeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    #annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",'emphasis', 'censored'},\n",
        "    annotate={\"hashtag\"},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "metadata": {
        "id": "U6KAmA4vgO4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Contoh\n",
        "def bersih_data(text):\n",
        "    return \" \".join(text_processor.pre_process_doc(text))\n",
        "\n",
        "def non_ascii(text):\n",
        "    return text.encode('ascii', 'replace').decode('ascii')"
      ],
      "metadata": {
        "id": "tTPbR8sxgUnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "final_string = []\n",
        "s = \"\"\n",
        "for text in df['Tweet'].values:\n",
        "    filteredSentence = []\n",
        "    EachReviewText = \"\"\n",
        "    proc = bersih_data(proc)\n",
        "    #     dst\n",
        "    #     dst\n",
        "    #     dst\n",
        "    EachReviewText = proc\n",
        "    final_string.append(EachReviewText)"
      ],
      "metadata": {
        "id": "UoXn5uPsgZUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"step01\"] = final_string"
      ],
      "metadata": {
        "id": "0YqfBINYgh-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Tampilkan posisi data terakhir (10 Teratas)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "r0DzgelAgky7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hapus = df[~df['step01'].str.contains(\" \")]"
      ],
      "metadata": {
        "id": "menvY-uCgnUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hapus.info()"
      ],
      "metadata": {
        "id": "tU4q6j6RgqAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hapus.head(10)"
      ],
      "metadata": {
        "id": "fHXdZJiogtDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = df[~df.isin(df_hapus)].dropna()"
      ],
      "metadata": {
        "id": "r6KVKXytguwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.info()"
      ],
      "metadata": {
        "id": "7ioyC9DDgxhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new"
      ],
      "metadata": {
        "id": "3-vd27wogzyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Bisa menggunakan nltk \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize "
      ],
      "metadata": {
        "id": "0SXEW5mJg1er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new['tokens'] = df['step01'].apply(word_tokenize_wrapper)"
      ],
      "metadata": {
        "id": "MxLAUsMmg4LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.head(10)"
      ],
      "metadata": {
        "id": "-1oBiejQg60h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_word = pd.read_csv('kamus_clean.csv', sep=\",\")\n",
        "### dst"
      ],
      "metadata": {
        "id": "SRKRU3eSg9bC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new['final_tokens'] = df_new['tokens'].apply(normalized_term)"
      ],
      "metadata": {
        "id": "TD_7dkM_hAST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "final_string_tokens = []\n",
        "for text in df_new['final_tokens'].values:\n",
        "    EachReviewText = \"\"\n",
        "    EachReviewText = ' '.join(text)\n",
        "    final_string_tokens.append(EachReviewText)"
      ],
      "metadata": {
        "id": "YO-444lwhEAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new[\"step02\"] = final_string_tokens"
      ],
      "metadata": {
        "id": "Oa-6OrDlhGB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Sampai tahap ini didaparkan kolom baru untuk Kolom `final_token` `dan step02`\n",
        "df_new.head(10)"
      ],
      "metadata": {
        "id": "SlmWpns9hHy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.to_csv('clean_dataset_uts_part01.csv',sep=\";\")"
      ],
      "metadata": {
        "id": "WXZTlm5ohNu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ],
      "metadata": {
        "id": "hUbgZFXzhSX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        " \n",
        "factory = StopWordRemoverFactory()\n",
        "\n",
        "more_stopword = ['sih', 'nya','rt','loh','lah', 'dd', 'mah', 'nye', 'eh', 'ehh', 'ah', 'yang']\n",
        " \n",
        "# Tambahkan Stopword Baru\n",
        "### Tuliskan perintahnya disini ???\n",
        "\n",
        "stopwords_sastrawi = factory.create_stop_word_remover()"
      ],
      "metadata": {
        "id": "EkN7s8HGhVDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new['step02'] = df_new['step02'].apply(str)"
      ],
      "metadata": {
        "id": "cTXf8KoOhYOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.head()"
      ],
      "metadata": {
        "id": "xU_seKGRhaVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "final_string = []\n",
        "s = \"\"\n",
        "for sentence in df_new[\"step02\"].values:\n",
        "    filteredSentence = []\n",
        "    EachReviewText = \"\"\n",
        "    st = stopwords_sastrawi.remove(sentence)\n",
        "    s = (stemmer.stem(st))\n",
        "    filteredSentence.append(s)\n",
        "    \n",
        "    EachReviewText = ' '.join(filteredSentence)\n",
        "    final_string.append(EachReviewText)"
      ],
      "metadata": {
        "id": "9i4XkSoEhcYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.loc[:, ('ProcessedText')] = final_string"
      ],
      "metadata": {
        "id": "ABSS13gPhfew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.head()"
      ],
      "metadata": {
        "id": "pAVWFm7jhkVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.to_csv('clean_dataset_uts_part02.csv',sep=\";\")"
      ],
      "metadata": {
        "id": "EZDwnMhchmC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_feature = df_new['ProcessedText']"
      ],
      "metadata": {
        "id": "mX5OiUYYhpNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_label = df_new['Emosi']"
      ],
      "metadata": {
        "id": "flVtB7wihuX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_label.value_counts()"
      ],
      "metadata": {
        "id": "KtLQ2kzQhxYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_label.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'"
      ],
      "metadata": {
        "id": "beVNZPyFhzbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the target variable\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.displot(dataset_label, label=f'target, skew: {dataset_label.skew():.2f}')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6uf--NAqh2pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "77elXOKdh51K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "6R6OHXsPh9dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_low_reviews = df_new[(df_new[\"Emosi\"] == 1)]\n",
        "negative_low_reviews = df_new[(df_new[\"Emosi\"] == -1)]\n",
        "positive_high_reviews = df_new[(df_new[\"Emosi\"] == 2)]\n",
        "negative_high_reviews = df_new[(df_new[\"Emosi\"] == -2)]"
      ],
      "metadata": {
        "id": "imLN8Z_4h_FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_low_reviews.head()"
      ],
      "metadata": {
        "id": "Cpif5bKfiA0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Positive_1_tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
        "Positive_1_tf_idf = Positive_1_tf_idf_vect.fit_transform(positive_low_reviews[\"ProcessedText\"].values)"
      ],
      "metadata": {
        "id": "rNXP0afsiD8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Positive_1_tf_idf.shape"
      ],
      "metadata": {
        "id": "gUTx4yHxiGSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = Positive_1_tf_idf_vect.get_feature_names()\n",
        "\n",
        "idfValues = Positive_1_tf_idf_vect.idf_\n",
        "\n",
        "d = dict(zip(features, 9 - idfValues))\n",
        "\n",
        "sortedDict = sorted(d.items(), key = lambda d: d[1], reverse = True)\n",
        "\n",
        "for i in range(200):\n",
        "    print(sortedDict[i])"
      ],
      "metadata": {
        "id": "zIcCZ_VAiIGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plot"
      ],
      "metadata": {
        "id": "qgTJWslAiLg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PlotWordCloud(frequency):\n",
        "    worcloudPlot = WordCloud(background_color=\"white\", width=1500, height=1000)\n",
        "    worcloudPlot.generate_from_frequencies(frequencies=frequency)\n",
        "    plot.figure(figsize=(15,10))\n",
        "    plot.imshow(worcloudPlot, interpolation=\"bilinear\")\n",
        "    plot.axis(\"off\")\n",
        "    plot.show()"
      ],
      "metadata": {
        "id": "UHQvCxFHiN4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PlotWordCloud(d)"
      ],
      "metadata": {
        "id": "XzTbYI5viQ3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Lengkapi kodenya\n",
        "PlotWordCloud(d)"
      ],
      "metadata": {
        "id": "SBUofNFRiSt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vect = TfidfVectorizer(ngram_range = (1,2))"
      ],
      "metadata": {
        "id": "-t6nmkSwiWx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_TFIDF = tfidf_vect.fit_transform(df_new[\"ProcessedText\"].values)"
      ],
      "metadata": {
        "id": "aQY3T32fifye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_TFIDF.shape"
      ],
      "metadata": {
        "id": "rxV1J2GUimi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Lengkapi kodenya\n",
        "train_TFIDF.shape, test_TFIDF.shape, train_labels_TFIDF.shape, test_labels_TFIDF.shape"
      ],
      "metadata": {
        "id": "UgujBDDBipJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier_nb = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)"
      ],
      "metadata": {
        "id": "ST_y6m6disqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_nb.fit(train_TFIDF, train_labels_TFIDF.ravel())"
      ],
      "metadata": {
        "id": "BKJ_GXuGivx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "sns.heatmap(confusion_matrix(test_labels_TFIDF, y_pred_nb_test), annot=True, cmap = 'viridis', fmt='.0f')\n",
        "plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
        "plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
        "plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LwaTMMX0ixy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "          ('Naive Bayes Multinomial', accuracy_nb_train, accuracy_nb_test),                    \n",
        "         ]"
      ],
      "metadata": {
        "id": "jqd_Ace2i25q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict = pd.DataFrame(data = models, columns=['Model', 'Training Accuracy', 'Test Accuracy'])\n",
        "predict"
      ],
      "metadata": {
        "id": "YvBq1mMti7Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_comparison = [\n",
        "                        ('Naive Bayes Multinomial', accuracy_nb_test, recall_nb_test, precision_nb_test),                          \n",
        "                    ]"
      ],
      "metadata": {
        "id": "vetYYxNbi8_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparison = pd.DataFrame(data = models_comparison, columns=['Model', 'Accuracy', 'Recall', 'Precision'])\n",
        "comparison"
      ],
      "metadata": {
        "id": "X9rhCDVJjADV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "f, axes = plt.subplots(2,1, figsize=(14,10))\n",
        "\n",
        "predict.sort_values(by=['Training Accuracy'], ascending=False, inplace=True)\n",
        "\n",
        "sns.barplot(x='Training Accuracy', y='Model', data = predict, palette='Blues_d', ax = axes[0])\n",
        "#axes[0].set(xlabel='Region', ylabel='Charges')\n",
        "axes[0].set_xlabel('Training Accuracy', size=16)\n",
        "axes[0].set_ylabel('Model')\n",
        "axes[0].set_xlim(0,1.0)\n",
        "axes[0].set_xticks(np.arange(0, 1.1, 0.1))\n",
        "\n",
        "predict.sort_values(by=['Test Accuracy'], ascending=False, inplace=True)\n",
        "\n",
        "sns.barplot(x='Test Accuracy', y='Model', data = predict, palette='Greens_d', ax = axes[1])\n",
        "#axes[0].set(xlabel='Region', ylabel='Charges')\n",
        "axes[1].set_xlabel('Test Accuracy', size=16)\n",
        "axes[1].set_ylabel('Model')\n",
        "axes[1].set_xlim(0,1.0)\n",
        "axes[1].set_xticks(np.arange(0, 1.1, 0.1))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SIwC1Cb7jCy5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}